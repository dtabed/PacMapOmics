{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ou_oWcr04V"
      },
      "outputs": [],
      "source": [
        "!pip install pacmap\n",
        "import pacmap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "\n",
        "# loading dataset\n",
        "\n",
        "#read the features\n",
        "x = read_csv('X1.csv')\n",
        "\n",
        "X = x.values\n",
        "X = X.transpose()\n",
        "print(X.shape)\n",
        "\n",
        "#read the labels\n",
        "y = read_csv('Y1.csv')\n",
        "\n",
        "Y = y.values\n",
        "# defining and initiliazing the PacMap\n",
        "embedding = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)\n",
        "\n",
        "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
        "X_transformed = embedding.fit_transform(X, init=\"pca\")\n",
        "print(X_transformed)\n",
        "# visualize the embedding\n",
        "#fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "#ax.scatter(X_transformed[:, 0], X_transformed[:, 1], cmap=\"Spectral\", c=y, s=0.6)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The part of the code has been taken from https://github.com/NaziaFatima/iSOM_GSN\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon May  6 15:20:11 2019\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "#    run_colorsExample()\n",
        "    import pandas as pd\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    import os, sys\n",
        "\n",
        "#    pos = [[9.5, 7.794228634059948],  [0, 5.196152422706632],  [9.5, 2.598076211353316],  [9, 5.196152422706632],  [6.5, 2.598076211353316],  [4.5, 7.794228634059948],  [0, 3.4641016151377553],  [9, 0.0],  [2, 6.9282032302755105],  [5.5, 7.794228634059948],  [5, 0.0],  [4, 5.196152422706632],  [0, 6.9282032302755105],  [1, 1.7320508075688776],  [0, 6.9282032302755105]]\n",
        "    pos = [[0.5, 7.794228634059948], [9, 6.9282032302755105], [3, 0.0], [5.5, 0.8660254037844388], [0, 5.196152422706632], [5, 5.196152422706632], [9, 5.196152422706632], [0.5, 2.598076211353316], [7.5, 7.794228634059948], [4.5, 7.794228634059948], [0, 0.0], [9, 0.0], [9.5, 7.794228634059948], [9.5, 2.598076211353316]]\n",
        "#    gene_list_top20 =['SPOP', 'TP53',  'FOXA1',  'CTNNB1',  'MED12',  'C16orf62',  'CLPTM1L',  'DPYSL2',  'NEIL1',  'SLC27A4',  'PITPNM2',  'PTEN', 'ATM',  'EMG1',  'ETV3',  'BRAF',  'NKX3-1',  'ZMYM3',  'OR4P4',  'SALL1']\n",
        "    gene_list_top14 = ['RUNX1', 'PIK3CA', 'GATA3', 'FOXA1', 'SF3B1', 'PTEN', 'CBFB', 'CDH1', 'MAP2K4', 'MAP3K1', 'ERBB2', 'NCOR1', 'FAM86B2', 'CDKN1B']\n",
        "    gene_list = gene_list_top14\n",
        "#    features_selected_withprefix_GE = ['GE_' + feature for feature in gene_list]\n",
        "#    features_selected_withprefix_DM = ['DM_' + feature for feature in gene_list]\n",
        "#    features_selected_withprefix_CN = ['CNA_' + feature for feature in gene_list]\n",
        "#    column_list = ['']\n",
        "#    column_list = column_list.append(pd.Index(features_selected_withprefix_GE))\n",
        "#    column_list = column_list.append(pd.Index(features_selected_withprefix_DM))\n",
        "#    column_list = column_list.append(pd.Index(features_selected_withprefix_CN))\n",
        "\n",
        "    pathGE = \"C:\\\\PR_Proj_Thesis\\\\NEW_SOM_Approach\\\\BRCA\\\\Mergefile_top20_norm.csv\"\n",
        "    # Read file\n",
        "    df = pd.read_csv(pathGE)\n",
        "\n",
        "    df1 = df.loc[df['TUMOR_STAGE'] == 'Stage IIIA']\n",
        "#    labels = df1['TUMOR_STAGE'].values\n",
        "#\n",
        "    df2 = df.loc[df['TUMOR_STAGE'] == 'Stage IIA']\n",
        "#    labels2 = df2['TUMOR_STAGE'].values\n",
        "#\n",
        "    df3 = df.loc[df['TUMOR_STAGE'] == 'Stage IIB']\n",
        "#    labels3 = df3['TUMOR_STAGE'].values\n",
        "\n",
        "    df1.drop(['TUMOR_STAGE','PATIENT_ID'],axis =1,inplace = True)\n",
        "    df2.drop(['TUMOR_STAGE','PATIENT_ID'],axis =1,inplace = True)\n",
        "    df3.drop(['TUMOR_STAGE','PATIENT_ID'],axis =1,inplace = True)\n",
        "\n",
        "    # applying scaling to make values between some range 0-1/-1-2 ,as need for Kohens SOM\n",
        "    scaler = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
        "    df1 = pd.DataFrame(scaler.fit_transform(df1), columns=df1.columns)\n",
        "    df2 = pd.DataFrame(scaler.fit_transform(df2), columns=df2.columns)\n",
        "    df3 = pd.DataFrame(scaler.fit_transform(df3), columns=df3.columns)\n",
        "\n",
        "    col_num = 14\n",
        "    #Create node list of numbers then create lables and dict\n",
        "    node_list = [i for i in range(col_num)]\n",
        "    new_lbl = [str(j) for j in range(col_num)]\n",
        "    new_lbl_dict = dict(enumerate(new_lbl))\n",
        "\n",
        "    # loop through all rows and get RGB values for each patient  ----- 3A\n",
        "    row_count = df1.shape[0]\n",
        "\n",
        "    for i in range(row_count): #loop through all rows\n",
        "        nodecolor =[]\n",
        "        for j in range(len(gene_list)): # loop through all nodes (i.e 14 nodes)\n",
        "\n",
        "            r_prefix = 'GE_'+gene_list[j]\n",
        "            g_prefix = 'DM_'+gene_list[j]\n",
        "            b_prefix = 'CNA_'+gene_list[j]\n",
        "\n",
        "            #Check if tht column exixts\n",
        "\n",
        "            R =  round(df1[r_prefix][i],5) if (r_prefix in df1.columns) else 0.0\n",
        "            G =  round(df1[g_prefix][i],5) if (g_prefix in df1.columns) else 0.0\n",
        "            B =  round(df1[b_prefix][i],5) if (b_prefix in df1.columns) else 0.0\n",
        "\n",
        "            nodecolor.append([R,G,B])\n",
        "\n",
        "        G=nx.chvatal_graph()\n",
        "        try:\n",
        "            nx.draw_networkx_nodes(G,pos,\n",
        "                               nodelist=node_list,\n",
        "                               edgecolors = [0,0,0],\n",
        "                               node_color = nodecolor,\n",
        "                               node_size=500,\n",
        "                               alpha=0.8)\n",
        "\n",
        "            nx.draw_networkx_labels(G,pos,new_lbl_dict,font_size=10)\n",
        "\n",
        "            plt.axis('on')\n",
        "            filename = str(i)+'_Template.png'\n",
        "            path = \"./training/3A\"\n",
        "            plt.savefig(os.path.join(path,filename))#, bbox_inches='tight', dpi=72)\n",
        "        except:\n",
        "            print(\"row = \",i,\" node_color = \",nodecolor)\n",
        "\n",
        "        print((\"\\r Preparing Images... \"+str(int(i*100.0/row_count))+\"%\" ), end=' ')\n",
        "    print(\"Done ---- 3A!!\")\n",
        "# loop through all rows and get RGB values for each patient  ----- 2A\n",
        "    row_count = df2.shape[0]\n",
        "\n",
        "    for i in range(row_count): #loop through all rows\n",
        "        nodecolor =[]\n",
        "        for j in range(len(gene_list)): # loop through all nodes (i.e 14 nodes)\n",
        "\n",
        "            r_prefix = 'GE_'+gene_list[j]\n",
        "            g_prefix = 'DM_'+gene_list[j]\n",
        "            b_prefix = 'CNA_'+gene_list[j]\n",
        "\n",
        "            #Check if tht column exixts\n",
        "\n",
        "            R =  round(df2[r_prefix][i],5) if (r_prefix in df2.columns) else 0.0\n",
        "            G =  round(df2[g_prefix][i],5) if (g_prefix in df2.columns) else 0.0\n",
        "            B =  round(df2[b_prefix][i],5) if (b_prefix in df2.columns) else 0.0\n",
        "\n",
        "            nodecolor.append([R,G,B])\n",
        "\n",
        "        G=nx.chvatal_graph()\n",
        "        try:\n",
        "            nx.draw_networkx_nodes(G,pos,\n",
        "                               nodelist=node_list,\n",
        "                               edgecolors = [0,0,0],\n",
        "                               node_color = nodecolor,\n",
        "                               node_size=500,\n",
        "                               alpha=0.8)\n",
        "\n",
        "            nx.draw_networkx_labels(G,pos,new_lbl_dict,font_size=10)\n",
        "\n",
        "            plt.axis('on')\n",
        "            filename = str(i)+'_Template.png'\n",
        "            path = \"./training/2A\"\n",
        "            plt.savefig(os.path.join(path,filename))#, bbox_inches='tight', dpi=72)\n",
        "        except:\n",
        "            print(\"row = \",i,\" node_color = \",nodecolor)\n",
        "        print((\"\\r Preparing Images... \"+str(int(i*100.0/row_count))+\"%\" ), end=' ')\n",
        "    print(\"Done ---- 2A!!\")\n",
        "# loop through all rows and get RGB values for each patient  ----- 2B\n",
        "    row_count = df3.shape[0]\n",
        "\n",
        "    for i in range(row_count): #loop through all rows\n",
        "        nodecolor =[]\n",
        "        for j in range(len(gene_list)): # loop through all nodes (i.e 14 nodes)\n",
        "\n",
        "            r_prefix = 'GE_'+gene_list[j]\n",
        "            g_prefix = 'DM_'+gene_list[j]\n",
        "            b_prefix = 'CNA_'+gene_list[j]\n",
        "\n",
        "            #Check if tht column exixts\n",
        "\n",
        "            R =  round(df3[r_prefix][i],5) if (r_prefix in df3.columns) else 0.0\n",
        "            G =  round(df3[g_prefix][i],5) if (g_prefix in df3.columns) else 0.0\n",
        "            B =  round(df3[b_prefix][i],5) if (b_prefix in df3.columns) else 0.0\n",
        "\n",
        "            nodecolor.append([R,G,B])\n",
        "\n",
        "        G=nx.chvatal_graph()\n",
        "        try:\n",
        "            nx.draw_networkx_nodes(G,pos,\n",
        "                               nodelist=node_list,\n",
        "                               edgecolors = [0,0,0],\n",
        "                               node_color = nodecolor,\n",
        "                               node_size=500,\n",
        "                               alpha=0.8)\n",
        "\n",
        "            nx.draw_networkx_labels(G,pos,new_lbl_dict,font_size=10)\n",
        "\n",
        "            plt.axis('on')\n",
        "            filename = str(i)+'_Template.png'\n",
        "            path = \"./training/2B\"\n",
        "            plt.savefig(os.path.join(path,filename))#, bbox_inches='tight', dpi=72)\n",
        "        except:\n",
        "            print(\"row = \",i,\" node_color = \",nodecolor)\n",
        "        print((\"\\r Preparing Images... \"+str(int(i*100.0/row_count))+\"%\" ), end=' ')\n",
        "\n",
        "print(\"Done ---- whole!!\")"
      ],
      "metadata": {
        "id": "0HiPvHTDr834"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Mar 13 04:46:46 2019\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "def Movefiles(source,destination,num):\n",
        "\n",
        "    # get list of files from Source(training) folder\n",
        "    list_of_file = [os.path.abspath(os.path.join(source,x)) for x in os.listdir(source)]#os.path.abspath(x)\n",
        "    #Randomly select files and put it in test folder\n",
        "    for i in range(num):\n",
        "        cut = random.choice(list_of_file)\n",
        "        shutil.move(cut, destination)\n",
        "        list_of_file = [os.path.abspath(os.path.join(source,x)) for x in os.listdir(source)]#os.path.abspath(x)\n",
        "    print ('Done moving '+ str(num) +' files from source : ' ,source, ' to destination :',destination)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num = 30\n",
        "    src_2A = 'PacMAp\\\\training\\\\2A'\n",
        "    dst_2A = 'PacMAp\\\\test\\\\2A'\n",
        "\n",
        "    src_2B = 'PacMap\\\\training\\\\2B'\n",
        "    dst_2B = 'PacMap\\\\test\\\\2B'\n",
        "\n",
        "    src_3A = 'PacMap\\\\training\\\\3A'\n",
        "    dst_3A = 'PacMap\\\\test\\\\3A'\n",
        "\n",
        "    Movefiles(src_2A,dst_2A,num)\n",
        "    Movefiles(src_2B,dst_2B,num)\n",
        "    Movefiles(src_3A,dst_3A,num)\n",
        "\n",
        "    print('*****************************DONE*************************************')"
      ],
      "metadata": {
        "id": "OtPAQUp9uzCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Feb 22 06:51:30 2019\n",
        "\"\"\"\n",
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D,BatchNormalization,Dropout\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "#Imports for collecting metrics\n",
        "import keras_metrics as km\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import tensorflow.keras as keras\n",
        "\n",
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2),strides = (2,2)))\n",
        "classifier.add(Dropout(0.2))\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2),strides = (2,2)))\n",
        "classifier.add(Dropout(0.5))\n",
        "### Adding a 3rd convolutional layer\n",
        "#classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "#classifier.add(BatchNormalization())\n",
        "#classifier.add(MaxPooling2D(pool_size = (2, 2),strides = (2,2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "## Adding a 4th convolutional layer\n",
        "#classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "##classifier.add(BatchNormalization())\n",
        "#classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "#classifier.add(Dropout(0.2))\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.2))\n",
        "#classifier.add(Dense(output_dim =1, activation = 'sigmoid'))# binary\n",
        "classifier.add(Dense(output_dim =3, activation = 'softmax')) # catgorical\n",
        "# SET METRICS\n",
        "\n",
        "precision = km.categorical_precision()\n",
        "recall = km.categorical_recall()\n",
        "f1= km.categorical_f1_score()\n",
        "\n",
        "# Compiling the CNN\n",
        "#classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy',precision,recall,f1])\n",
        "# # checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator()\n",
        "#                                    rescale = 1./255,\n",
        "#                                   shear_range = 0.2,\n",
        "#                                   zoom_range = 0.2,\n",
        "#                                   horizontal_flip = True,\n",
        "#                                   validation_split = 0.7)\n",
        "\n",
        "test_datagen = ImageDataGenerator()#rescale = 1./255)\n",
        "\n",
        "seed =7\n",
        "training_set = train_datagen.flow_from_directory('training',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 shuffle =True,\n",
        "                                                 seed =seed)#     seed =seed)#,save_to_dir = 'generatedimages') #categorical,binary\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('test',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size =32,\n",
        "                                            class_mode = 'categorical',\n",
        "                                            shuffle =False)#,   seed =seed)#categorical,binary\n",
        "with tf.Session() as s:\n",
        "    s.run(tf.global_variables_initializer())\n",
        "    classifier.fit_generator(training_set,\n",
        "                         samples_per_epoch = 150,\n",
        "                         nb_epoch =30,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 60,\n",
        "                         shuffle =True,\n",
        "                         callbacks=callbacks_list,\n",
        "                         verbose = 2)\n"
      ],
      "metadata": {
        "id": "MIiTyh1VvWp2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}